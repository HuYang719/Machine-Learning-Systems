# Distributed Deep Learning in Open Collaboration

## Motivation

Use internet-based computation resources to accelerate model training

Comment: different with us: 1.they are for large-scale public dataset, we are fin-tuning; 2. not target on large-scale model; 



## Comments

Can we use the  slice of time to collabrate  

This paper only introduces the data parallelism, however, how to balance data paralllism and model parallelism together, it is another question.

