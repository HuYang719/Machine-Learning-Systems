# Survey for Decentralized Machine Learning Algorithm

\* : related to topology directly

^: Heterogeneous conditions 

o: Low bandwidth/High latency 

| Title                                                        | Team              | Venue and Year | Problem                                                      | Method                                                       | Result                                                       | Notes |
| ------------------------------------------------------------ | ----------------- | -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ----- |
| [BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning](https://arxiv.org/abs/2111.04287) | UCLA              | 2021           | Framework                                                    | Unified abstraction of communication operations, system-level computation acceleration | 1.2×∼1.8× speedup over Horovod                               |       |
| [Consensus Control for Decentralized Deep Learning](http://proceedings.mlr.press/v139/kong21a/kong21a.pdf) | EPFL              | PMLR, 2021     | DSGD training performance impacted by network size, communication topology and data partitioning. | Identify the changing consensus distance                     | practical training guidelines                                | *     |
| [Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices](https://proceedings.neurips.cc/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf) | Yandex            | NIPS, 2021     | DSGD for unreliable devices                                  | Design training algorithm                                    | 1.5x training speedup  on preemptible compute nodes.         | ^     |
| [D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology](https://arxiv.org/pdf/2104.07365.pdf) | EPFL              | 2021           | Data heterogeneity in DSGD                                   | Design network topology                                      | require less communication for large nodes                   | *     |
| [Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce](https://dl-acm-org.libproxy2.usc.edu/doi/abs/10.1145/3448016.3452773?casa_token=SxNmuPdNRewAAAAA:xF5wp_YbCA9jzRe8LXx1xjO8u8_1xe6C6UH_0ZzVCq5Xs49CKKNh90U5Wjmlo5Fs58Cl8jKv2AUizGo) | Peking University | SIGMOD, 2021   | stragglers and communication delays                          | Algorithm with parallel-asynchronous partial-reduce operations | 1.21×-2× faster than SOTA                                    | ^     |
| [Decentralized Federated Learning: Balancing Communication and Computing Costs](https://arxiv.org/pdf/2107.12048.pdf) | USTC              | 2022           | Framework for balance communication and computing costs      | Design local update and inter-node communication             | Outperform D-SGD and enhances communication efficiency       |       |
| [DLion: Decentralized Distributed Deep Learning in Micro-Clouds](https://dl-acm-org.libproxy2.usc.edu/doi/pdf/10.1145/3431379.3460643) | UMN               | HDPC,2021      | DSGD performance                                             | Weighted dynamic batching, Per-link prioritized gradient, Direct knowledge transfer | Outperform four SOTA distributed DL systems                  |       |
| [Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD](https://arxiv.org/pdf/2105.08023.pdf) | DAMO, Alibaba     | 2021           | D-SGD suffers from slow convergence for large and sparse networks | Eliminating the influence of data heterogeneity              | Remove the data heterogeneity  can ameliorate the network topology dependence | *     |
| [BlueConnect: Decomposing All-Reduce for Deep Learning on Heterogeneous Network Hierarchy](https://proceedings.mlsys.org/paper/2019/hash/9b8619251a19057cff70779273e95aa6-Abstract.html) | IBM               | MLSys, 2019    | Communication Framework                                      | Decompose all-reduce operation                               | Reduce synchronization overhead by 87% on 192 GPUs           | ^     |
| [Efficient Communication Topology via Partially Differential Privacy for Decentralized Learning](https://ieeexplore-ieee-org.libproxy2.usc.edu/stamp/stamp.jsp?tp=&arnumber=9522327) | NCCU, Taiwan      | ICCCN, 2021    | Communication Topology                                       | Optimization                                                 | outperform SOTA in convergence rate,  physical training time | *     |
| [Impact of Network Topology on the Convergence of Decentralized Federated Learning Systems](https://ieeexplore.ieee.org/abstract/document/9631460/?casa_token=95Od-1YNPq8AAAAA:_hD90Wu3c-h5hvRzTMva3re03sAVap4zYqJl2TNWXtd4G_gkMIlAbWcV5lNxFsNNdIYO3uS-Dn8) | ISTI-CNR          | ISCC, 2021     | Training performance v.s. communication topology             | empirical study                                              | converge faster in small-scale clusters                      | *     |
|                                                              |                   |                |                                                              |                                                              |                                                              |       |
|                                                              |                   |                |                                                              |                                                              |                                                              |       |
|                                                              |                   |                |                                                              |                                                              |                                                              |       |
|                                                              |                   |                |                                                              |                                                              |                                                              |       |
|                                                              |                   |                |                                                              |                                                              |                                                              |       |
|                                                              |                   |                |                                                              |                                                              |                                                              |       |

