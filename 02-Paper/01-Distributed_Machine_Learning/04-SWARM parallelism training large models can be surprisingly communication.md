# SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-efficient



## Problem，Motivation， Importance

- Problem: Training large models (13B parameters) for unreliable devices under low bandwidth (400mbps)

- Motivation: HPC in data center is notoriously expensive for researchers. Use more cost-efficient distributed training /the help of volunteers to train large models
- Importance: let everyone has the ability to train large-scale model!  

## Method

- Suqire-cube law
- Pipeline parallelism for unrealiable connection
- Compression-aware architectures
- 

## Experiments



## Reviewer Comments



## Writing



## My Comments

**In fact, we can estimate the training time with joining users and the model and data size.**

#### Evaluation (Novelty, Effective, Problem Size )

Problem Size: 100

- Solve this can make more internet-based devices join in training
- Different from the data center scenario

Novelty: 1

- Random wire seems werid 
- Compression model is a common technique

Effective: 



#### What I can Learn

#### Which part could be improved

#### Future directions

